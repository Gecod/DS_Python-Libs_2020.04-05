{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score as r2, accuracy_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import rcParams\n",
    "# matplotlib.rcParams.update({'font.size': 10})\n",
    "pd.options.display.max_columns = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание датасета**\n",
    "\n",
    "* **Id** - идентификационный номер квартиры\n",
    "* **DistrictId** - идентификационный номер района\n",
    "* **Rooms** - количество комнат\n",
    "* **Square** - площадь\n",
    "* **LifeSquare** - жилая площадь\n",
    "* **KitchenSquare** - площадь кухни\n",
    "* **Floor** - этаж\n",
    "* **HouseFloor** - количество этажей в доме\n",
    "* **HouseYear** - год постройки дома\n",
    "* **Ecology_1, Ecology_2, Ecology_3** - экологические показатели местности\n",
    "* **Social_1, Social_2, Social_3** - социальные показатели местности\n",
    "* **Healthcare_1, Helthcare_2** - показатели местности, связанные с охраной здоровья\n",
    "* **Shops_1, Shops_2** - показатели, связанные с наличием магазинов, торговых центров\n",
    "* **Price** - цена квартиры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приведение типов данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Id'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Id'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Id'] = train['Id'].astype(str)\n",
    "train['Id'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Id'] = test['Id'].astype(str)\n",
    "test['Id'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что не нравится и хотелось бы скорректировать:\n",
    "\n",
    "1. приведение категориальных признаков Ecology_2, Ecology_3, Shops_2 в бинарные\n",
    "* некорректные значения HouseYear в train\n",
    "* недостаток данных Healthcare_1\n",
    "* недостаток данных LifeSquare\n",
    "* max KitchenSquare выглядит слишком большим в обоих датасетах\n",
    "* max LifeSquare выглядит слишком большим в train\n",
    "* min LifeSquare выглядит слишком малым в обоих датасетах\n",
    "\n",
    "Что касается площадей - в моем понимании Square > KitchenSquare + LifeSquare. Исходя из этой логики в дальнейшем будем пытаться скорректировать значения в датасетах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Приведение категориальных признаков Ecology_2, Ecology_3, Shops_2 в бинарные\n",
    "Проверим количество возможных значений в категориальных признаках Ecology_2, Ecology_3, Shops_2 датасета train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['Ecology_2'].value_counts()) \n",
    "print(train['Ecology_3'].value_counts()) \n",
    "print(train['Shops_2'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. каждый из категориальных признаков Ecology_2, Ecology_3, Shops_2 содержит только два варианта значений (A или B), то мы можем оставить только одно из них, которое будет соответствовать '1', а '0' будет соответствовать другому варианту. Заметим также, что в этих признаках нет пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train, \n",
    "                   pd.get_dummies(pd.DataFrame({'Ecology_2':train['Ecology_2'], \n",
    "                                                'Ecology_3':train['Ecology_3'], \n",
    "                                                'Shops_2':train['Shops_2']}), \n",
    "                                  prefix=['Ecol_2', 'Ecol_3', 'Shops_2'], \n",
    "                                  prefix_sep='_', \n",
    "                                  drop_first=True, \n",
    "                                  dtype='int64')\n",
    "                  ], axis=1)\n",
    "train.drop(['Ecology_2', 'Ecology_3', 'Shops_2'], axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично проверим количество возможных значений в категориальных признаках Ecology_2, Ecology_3, Shops_2 датасета test, а также убедимся в отсутствии пропусков, и проведем те же манипуляции над датасетом для приведения названных признаков в бинарные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test['Ecology_2'].value_counts()) \n",
    "print(test['Ecology_3'].value_counts()) \n",
    "print(test['Shops_2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, \n",
    "                   pd.get_dummies(pd.DataFrame({'Ecology_2':test['Ecology_2'], \n",
    "                                                'Ecology_3':test['Ecology_3'], \n",
    "                                                'Shops_2':test['Shops_2']}), \n",
    "                                  prefix=['Ecol_2', 'Ecol_3', 'Shops_2'], \n",
    "                                  prefix_sep='_', \n",
    "                                  drop_first=True, \n",
    "                                  dtype='int64')\n",
    "                  ], axis=1)\n",
    "test.drop(['Ecology_2', 'Ecology_3', 'Shops_2'], axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Исправим значение HouseYear в датасете train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['HouseYear'] > 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из предыдущего вывода видно 2 выброса c DistrictId 109 и 147. Т.к. это всего два значения, они не должны оказать сильного влияния на обучение, но, тем не менее являются выбросами. Предположу замену HouseYear в первом случае на 2011, а во втором на 1968."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['HouseYear'] == 20052011), 'HouseYear'] = 2011\n",
    "train.loc[(train['HouseYear'] == 4968), 'HouseYear'] = 1968\n",
    "pd.concat([train.loc[1497:1497,:], train.loc[4189:4189,:]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Заполним недостающие данные Healthcare_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассмотрим таблицу корреляций признака Healthcare_1\n",
    "train.corr()[['Healthcare_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из предыдущей таблицы и сведений о признаках предположу, что признак Healthcare_1 зависит от (в порядке убывания):\n",
    " - DistrictId\n",
    " - Social_1\n",
    " - Social_2\n",
    " - Social_3\n",
    " - Helthcare_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполняем пропуски Healthcare_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ident_model(train_df):\n",
    "    model_1 = RandomForestClassifier(max_depth=29)\n",
    "    train_ = train_df[train_df['Healthcare_1'].isna() == False][['DistrictId', 'Social_1', 'Social_2', 'Social_3', 'Helthcare_2','Healthcare_1']]\n",
    "    tr, tst = train_test_split(train_, test_size = 0.15, random_state=29)\n",
    "    model_1.fit(tr.drop('Healthcare_1', axis = 1), tr['Healthcare_1'])\n",
    "    pred = model_1.predict(tst.drop('Healthcare_1', axis = 1))\n",
    "    print(accuracy_score(tst['Healthcare_1'], pred))\n",
    "    test_ = train_df[train_df['Healthcare_1'].isna()][['DistrictId', 'Social_1', 'Social_2', 'Social_3', 'Helthcare_2','Healthcare_1']]\n",
    "    model_1.fit(train_.drop('Healthcare_1', axis = 1), train_['Healthcare_1'])\n",
    "    pred = model_1.predict(test_.drop('Healthcare_1', axis = 1))\n",
    "    return model_1\n",
    "\n",
    "\n",
    "def fill_H1(model_1, train_df):\n",
    "    test_ = train_df[train_df['Healthcare_1'].isna()][['DistrictId', 'Social_1','Social_2', 'Social_3', 'Helthcare_2','Healthcare_1']]\n",
    "    pred = model_1.predict(test_.drop('Healthcare_1', axis = 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = ident_model(train)\n",
    "pred = fill_H1(model_1, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пропуски в тренировочном датасете\n",
    "train.loc[train['Healthcare_1'].isna(), 'Healthcare_1'] = pred\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем тестовый датасет\n",
    "predict_test_H1 = fill_H1(model_1, test)\n",
    "test.loc[test['Healthcare_1'].isna(), 'Healthcare_1'] = predict_test_H1\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Заполним недостающие данные LifeSquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем LifeSquare как (Square - KitchenSquare) * (медиану отношения LifeSquare/(Square - KitchenSquare) для Square > LifeSquare\n",
    "def fill_LifeSquare(train_df):\n",
    "    train_df.loc[(train_df['Square'] < train_df['LifeSquare']) | (train_df['LifeSquare'].isna()), 'LifeSquare'] = \\\n",
    "    (train_df.loc[(train_df['Square'] < train_df['LifeSquare']) | (train_df['LifeSquare'].isna()), 'Square'] - \\\n",
    "    train_df.loc[(train_df['Square'] < train_df['LifeSquare']) | (train_df['LifeSquare'].isna()), 'KitchenSquare']) * \\\n",
    "    (train_df.loc[(train_df['Square'] > train_df['LifeSquare']), 'LifeSquare'] / \\\n",
    "    (train_df.loc[(train_df['Square'] > train_df['LifeSquare']), 'Square'] - \\\n",
    "    train_df.loc[(train_df['Square'] > train_df['LifeSquare']), 'KitchenSquare'])).median()\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пропуски в тренировочном датасете\n",
    "train = fill_LifeSquare(train)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# аналогично для тестового датасета\n",
    "test = fill_LifeSquare(test)\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заменяем HouseFloor\n",
    "def fill_HouseFloor(df):\n",
    "    df.loc[df['Floor'] > df['HouseFloor'], 'HouseFloor'] = df.loc[df['Floor'] > df['HouseFloor'], 'Floor']\n",
    "    return df\n",
    "\n",
    "train = fill_HouseFloor(train)\n",
    "test = fill_HouseFloor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#заполняем комнаты\n",
    "def fill_rooms(df):\n",
    "    # df.loc[(df['Rooms'] > 5) | (df['Rooms'] == 0), 'Rooms'] = 5\n",
    "    df.loc[(df['Rooms'] > 5), 'Rooms'] = 5\n",
    "    df.loc[(df['Rooms'] == 0), 'Rooms'] = 1\n",
    "    return df\n",
    "\n",
    "train = fill_rooms(train)\n",
    "test = fill_rooms(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем новый признак, средней цены на кв. метр по району"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['avg_price'] = train['Price']/train['Square']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean_by_Did = train[['DistrictId','avg_price']].groupby('DistrictId')[['avg_price']].median().reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean_by_Did"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_ = pd.merge(train.drop('avg_price', axis = 'columns'),mean_by_Did, how = 'left', left_on=['DistrictId'], right_on=['DistrictId'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# с признаком avg_price\n",
    "test_ = pd.merge(test_,mean_by_Did, how = 'left', left_on=['DistrictId'], right_on=['DistrictId'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_['avg_price'] = test_['avg_price'].fillna(test_['avg_price'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Прогнозирование"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Со всеми признаками\n",
    "rcParams['figure.figsize'] = 8, 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id','Price'], axis = 1), \n",
    "                                                    train['Price'], test_size = 0.2, random_state=29)\n",
    "\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "max_depth = list(range(11,19))\n",
    "for i in max_depth:\n",
    "    mod1 = RandomForestRegressor(max_depth=i, n_estimators=220)\n",
    "    mod1.fit(X_train,y_train)\n",
    "    pr_train = mod1.predict(X_train)\n",
    "    pr_test = mod1.predict(X_test)\n",
    "    r2_train.append(r2(y_train, pr_train))\n",
    "    r2_test.append(r2(y_test, pr_test))\n",
    "    \n",
    "imp = pd.DataFrame()\n",
    "imp['name'] = X_train.columns\n",
    "imp['value'] = mod1.feature_importances_\n",
    "print(imp.sort_values('value' , ascending = False))\n",
    "print(r2_test)\n",
    "print('Max valid R2 =', max(r2_test), '( train R2 =', r2_train[r2_test.index(max(r2_test))], ')')\n",
    "print('max_depth =', r2_test.index(max(r2_test))+(min(max_depth)))\n",
    "plt.scatter(max_depth, r2_train, c ='r')\n",
    "plt.scatter(max_depth, r2_test, c = 'b')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Удаляем признаки с наименьшей важностью\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id', 'Price', 'Ecol_2_B', 'Shops_2_B', 'Ecol_3_B', \n",
    "                                                                'Helthcare_2', 'Shops_1', 'HouseFloor'], \n",
    "                                                               axis = 1), \n",
    "                                                    train['Price'], test_size = 0.2, random_state=29)\n",
    "\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "max_depth = list(range(11,19))\n",
    "for i in max_depth:\n",
    "    mod1 = RandomForestRegressor(max_depth=i, n_estimators=220)\n",
    "    mod1.fit(X_train,y_train)\n",
    "    pr_train = mod1.predict(X_train)\n",
    "    pr_test = mod1.predict(X_test)\n",
    "    r2_train.append(r2(y_train, pr_train))\n",
    "    r2_test.append(r2(y_test, pr_test))\n",
    "    \n",
    "imp = pd.DataFrame()\n",
    "imp['name'] = X_train.columns\n",
    "imp['value'] = mod1.feature_importances_\n",
    "print(imp.sort_values('value' , ascending = False))\n",
    "print(r2_test)\n",
    "print('Max valid R2 =', max(r2_test), '( train R2 =', r2_train[r2_test.index(max(r2_test))], ')')\n",
    "print('max_depth =', r2_test.index(max(r2_test))+(min(max_depth)))\n",
    "plt.scatter(max_depth, r2_train, c ='r')\n",
    "plt.scatter(max_depth, r2_test, c = 'b')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "    'n_estimators': [170, 220, 255, 290, 320],\n",
    "    'max_features': np.arange(5, 9),\n",
    "    'max_depth': np.arange(11, 19),\n",
    "}\n",
    "\n",
    "rgr = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=29),\n",
    "    param_grid=parameters,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Со всеми признаками\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id','Price'], axis = 1), \n",
    "                                                    train['Price'], test_size = 0.16, random_state=29)\n",
    "rgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Для ознакомления с результатами Grid search можно использовать атрибут `.cv_results_`. Удобнее всего визуализировать эти результаты в виде `DataFrame`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv_results = pd.DataFrame(rgr.cv_results_)\n",
    "\n",
    "cv_results.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Посмотрим, как выбранные нами параметры влияют на точность модели:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param_columns = [\n",
    "    column\n",
    "    for column in cv_results.columns\n",
    "    if column.startswith('param_')\n",
    "]\n",
    "\n",
    "score_columns = ['mean_test_score']\n",
    "\n",
    "cv_results = (cv_results[param_columns + score_columns]\n",
    "              .sort_values(by=score_columns, ascending=False))\n",
    "\n",
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rgr.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Полученная в результате модель аналогична такой модели:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rgr = RandomForestRegressor(max_depth=18, max_features=5, n_estimators=320, random_state=29)\n",
    "\n",
    "rgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rgr.predict(X_test)\n",
    "\n",
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация: k-means и последующее выделение 3-х кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def apply_elbow_method(X):\n",
    "    distortions = []\n",
    "    K = range(1,10)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k, random_state=29).fit(X)\n",
    "        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_clusters_distribution(unique_labels, labels_counts):\n",
    "    plt.figure(figsize=(8,5))\n",
    "\n",
    "    plt.bar(unique, counts)\n",
    "\n",
    "    plt.xlabel('Clusters')\n",
    "    plt.xticks(unique)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Clusters distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apply_elbow_method(train.drop(['Id', 'Price'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_3 = KMeans(n_clusters=3, random_state=29)\n",
    "labels_clast_3 = kmeans_3.fit_predict(train.drop(['Id', 'Price'], axis = 1))\n",
    "labels_clast_3 = pd.Series(labels_clast_3, name='clusters_3')\n",
    "\n",
    "unique, counts = np.unique(labels_clast_3, return_counts=True)\n",
    "display_clusters_distribution(unique, counts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def reduce_dims_to_3D_space_with_PCA(df):\n",
    "    pca = PCA(n_components=3)\n",
    "    components = pca.fit_transform(df)\n",
    "    return pd.DataFrame(data = components, columns = ['component_1', 'component_2', 'component_3'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def display_components_in_3D_space(components_df, labels=None):\n",
    "    components_with_labels_df = pd.concat([components_df, labels], axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    if labels is not None:\n",
    "        ax.scatter(components_with_labels_df['component_1'], \n",
    "                   components_with_labels_df['component_2'], \n",
    "                   components_with_labels_df['component_3'], \n",
    "                   c=labels, cmap=plt.get_cmap('jet'), alpha=0.5)\n",
    "    else:\n",
    "        ax.scatter(components_with_labels_df['component_1'], \n",
    "                   components_with_labels_df['component_2'], \n",
    "                   components_with_labels_df['component_3'], \n",
    "                   alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('component_1')\n",
    "    ax.set_ylabel('component_2')\n",
    "    ax.set_zlabel('component_3')\n",
    "    plt.title('3D mapping of objects')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "components_3d = reduce_dims_to_3D_space_with_PCA(train.drop(['Id', 'Price'], axis = 1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from mpl_toolkits.mplot3d.axes3d import Axes3D"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display_components_in_3D_space(components_3d, labels_clast_3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clusters_3_dummies = pd.get_dummies(labels_clast_3, drop_first=True, prefix='clusters_3')\n",
    "clusters_3_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление новых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ext = pd.concat([train, labels_clast_3], axis=1)\n",
    "train_ext.sort_values(by=['clusters_3', 'Id'], ascending=True, inplace=True)\n",
    "train_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация тестового датасета и добавление в него новых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clast_3_test = kmeans_3.predict(test.drop(['Id'], axis = 1))\n",
    "labels_clast_3_test = pd.Series(labels_clast_3_test, name='clusters_3')\n",
    "\n",
    "unique, counts = np.unique(labels_clast_3_test, return_counts=True)\n",
    "display_clusters_distribution(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ext = pd.concat([test, labels_clast_3_test], axis=1)\n",
    "test_ext.sort_values(by=['clusters_3', 'Id'], ascending=True, inplace=True)\n",
    "test_ext"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Со всеми признаками\n",
    "# rcParams['figure.figsize'] = 8, 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ext[train_ext['clusters_3'] == 0].\n",
    "                                                    drop(['Id', 'Price', 'clusters_3'], axis = 1), \n",
    "                                                    train_ext.loc[train_ext['clusters_3'] == 0, 'Price'], \n",
    "                                                    test_size = 0.2, random_state=29)\n",
    "\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "max_depth = list(range(11, 22)) # 16\n",
    "# n_estimators = list(range(257, 264)) # 260\n",
    "for i in max_depth:\n",
    "# for i in n_estimators:\n",
    "    mod1 = RandomForestRegressor(max_depth=i, n_estimators=260, random_state=29)\n",
    "    mod1.fit(X_train,y_train)\n",
    "    pr_train = mod1.predict(X_train)\n",
    "    pr_test = mod1.predict(X_test)\n",
    "    r2_train.append(r2(y_train, pr_train))\n",
    "    r2_test.append(r2(y_test, pr_test))\n",
    "    \n",
    "imp = pd.DataFrame()\n",
    "imp['name'] = X_train.columns\n",
    "imp['value'] = mod1.feature_importances_\n",
    "print(imp.sort_values('value', ascending = False))\n",
    "print(r2_test)\n",
    "print('Max valid R2 =', max(r2_test), '( train R2 =', r2_train[r2_test.index(max(r2_test))], ')')\n",
    "print('max_depth =', max_depth[r2_test.index(max(r2_test))])\n",
    "# print('n_estimators =', n_estimators[r2_test.index(max(r2_test))])\n",
    "plt.scatter(max_depth, r2_train, c ='r')\n",
    "plt.scatter(max_depth, r2_test, c = 'b')\n",
    "# plt.scatter(n_estimators, r2_train, c ='r')\n",
    "# plt.scatter(n_estimators, r2_test, c = 'b')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max valid R2 = 0.7463025606261264 ( train R2 = 0.9473107664191112 )\n",
    "max_depth = 15"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Со всеми признаками\n",
    "# rcParams['figure.figsize'] = 8, 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ext[train_ext['clusters_3'] == 1].\n",
    "                                                    drop(['Id', 'Price', 'clusters_3'], axis = 1), \n",
    "                                                    train_ext.loc[train_ext['clusters_3'] == 1, 'Price'], \n",
    "                                                    test_size = 0.2, random_state=29)\n",
    "\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "max_depth = list(range(11, 22))\n",
    "# n_estimators = list(range(444, 451)) # 447\n",
    "for i in max_depth:\n",
    "# for i in n_estimators:\n",
    "    mod1 = RandomForestRegressor(max_depth=i, n_estimators=447, random_state=29)\n",
    "    mod1.fit(X_train,y_train)\n",
    "    pr_train = mod1.predict(X_train)\n",
    "    pr_test = mod1.predict(X_test)\n",
    "    r2_train.append(r2(y_train, pr_train))\n",
    "    r2_test.append(r2(y_test, pr_test))\n",
    "    \n",
    "imp = pd.DataFrame()\n",
    "imp['name'] = X_train.columns\n",
    "imp['value'] = mod1.feature_importances_\n",
    "print(imp.sort_values('value', ascending = False))\n",
    "print(r2_test)\n",
    "print('Max valid R2 =', max(r2_test), '( train R2 =', r2_train[r2_test.index(max(r2_test))], ')')\n",
    "print('max_depth =', max_depth[r2_test.index(max(r2_test))])\n",
    "# print('n_estimators =', n_estimators[r2_test.index(max(r2_test))])\n",
    "plt.scatter(max_depth, r2_train, c ='r')\n",
    "plt.scatter(max_depth, r2_test, c = 'b')\n",
    "# plt.scatter(n_estimators, r2_train, c ='r')\n",
    "# plt.scatter(n_estimators, r2_test, c = 'b')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max valid R2 = 0.5509655649093166 ( train R2 = 0.9290883513385566 )\n",
    "max_depth = 11"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Со всеми признаками\n",
    "# rcParams['figure.figsize'] = 8, 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ext[train_ext['clusters_3'] == 2].\n",
    "                                                    drop(['Id', 'Price', 'clusters_3'], axis = 1), \n",
    "                                                    train_ext.loc[train_ext['clusters_3'] == 2, 'Price'], \n",
    "                                                    test_size = 0.2, random_state=29)\n",
    "\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "max_depth = list(range(11, 22)) # 16\n",
    "# n_estimators = list(range(13, 20)) # 16\n",
    "for i in max_depth:\n",
    "# for i in n_estimators:\n",
    "    mod1 = RandomForestRegressor(max_depth=i, n_estimators=16, random_state=29)\n",
    "    mod1.fit(X_train,y_train)\n",
    "    pr_train = mod1.predict(X_train)\n",
    "    pr_test = mod1.predict(X_test)\n",
    "    r2_train.append(r2(y_train, pr_train))\n",
    "    r2_test.append(r2(y_test, pr_test))\n",
    "    \n",
    "imp = pd.DataFrame()\n",
    "imp['name'] = X_train.columns\n",
    "imp['value'] = mod1.feature_importances_\n",
    "print(imp.sort_values('value', ascending = False))\n",
    "print(r2_test)\n",
    "print('Max valid R2 =', max(r2_test), '( train R2 =', r2_train[r2_test.index(max(r2_test))], ')')\n",
    "print('max_depth =', max_depth[r2_test.index(max(r2_test))])\n",
    "# print('n_estimators =', n_estimators[r2_test.index(max(r2_test))])\n",
    "plt.scatter(max_depth, r2_train, c ='r')\n",
    "plt.scatter(max_depth, r2_test, c = 'b')\n",
    "# plt.scatter(n_estimators, r2_train, c ='r')\n",
    "# plt.scatter(n_estimators, r2_test, c = 'b')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max valid R2 = 0.7656705315953981 ( train R2 = 0.9479899543634576 )\n",
    "max_depth = 12"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Удаляем признаки с наименьшей важностью\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id', 'Price', 'Ecol_2_B', 'Shops_2_B', 'Ecol_3_B', \n",
    "                                                                'Helthcare_2', 'Shops_1', 'HouseFloor'], \n",
    "                                                               axis = 1), \n",
    "                                                    train['Price'], test_size = 0.2, random_state=29)\n",
    "\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "max_depth = list(range(11,19))\n",
    "for i in max_depth:\n",
    "    mod1 = RandomForestRegressor(max_depth=i, n_estimators=220)\n",
    "    mod1.fit(X_train,y_train)\n",
    "    pr_train = mod1.predict(X_train)\n",
    "    pr_test = mod1.predict(X_test)\n",
    "    r2_train.append(r2(y_train, pr_train))\n",
    "    r2_test.append(r2(y_test, pr_test))\n",
    "    \n",
    "imp = pd.DataFrame()\n",
    "imp['name'] = X_train.columns\n",
    "imp['value'] = mod1.feature_importances_\n",
    "print(imp.sort_values('value' , ascending = False))\n",
    "print(r2_test)\n",
    "print(max(r2_test))\n",
    "plt.scatter(max_depth, r2_train, c ='r')\n",
    "plt.scatter(max_depth, r2_test, c = 'b')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [100, 150, 200, 250, 300, 350, 400],\n",
    "    'max_features': np.arange(9, 19),\n",
    "    'max_depth': np.arange(11, 21),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr0 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=29),\n",
    "    param_grid=parameters,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Со всеми признаками\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id','Price'], axis = 1), \n",
    "#                                                     train['Price'], test_size = 0.16, random_state=29)\n",
    "clst_3_num = 0 # номер кластера\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ext[train_ext['clusters_3'] == clst_3_num].\n",
    "                                                    drop(['Id', 'Price', 'clusters_3'], axis = 1), \n",
    "                                                    train_ext.loc[train_ext['clusters_3'] == clst_3_num, 'Price'], \n",
    "                                                    test_size = 0.2, random_state=29)\n",
    "\n",
    "rgr0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr0.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr1 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=29),\n",
    "    param_grid=parameters,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Со всеми признаками\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id','Price'], axis = 1), \n",
    "#                                                     train['Price'], test_size = 0.16, random_state=29)\n",
    "clst_3_num = 1 # номер кластера\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ext[train_ext['clusters_3'] == clst_3_num].\n",
    "                                                    drop(['Id', 'Price', 'clusters_3'], axis = 1), \n",
    "                                                    train_ext.loc[train_ext['clusters_3'] == clst_3_num, 'Price'], \n",
    "                                                    test_size = 0.2, random_state=29)\n",
    "\n",
    "rgr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr2 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=29),\n",
    "    param_grid=parameters,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Со всеми признаками\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train.drop(['Id','Price'], axis = 1), \n",
    "#                                                     train['Price'], test_size = 0.16, random_state=29)\n",
    "clst_3_num = 2 # номер кластера\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ext[train_ext['clusters_3'] == clst_3_num].\n",
    "                                                    drop(['Id', 'Price', 'clusters_3'], axis = 1), \n",
    "                                                    train_ext.loc[train_ext['clusters_3'] == clst_3_num, 'Price'], \n",
    "                                                    test_size = 0.2, random_state=29)\n",
    "\n",
    "rgr2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученная в результате модель аналогична такой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr = RandomForestRegressor(max_depth=18, max_features=5, n_estimators=320, random_state=29)\n",
    "\n",
    "rgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rgr.predict(X_test)\n",
    "\n",
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели кластерно на всех данных и итоговое предсказание с найденными `best_params_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rgr_0 = RandomForestRegressor(max_depth=18, max_features=5, n_estimators=320, random_state=29)\n",
    "rgr_0.fit(train_ext[train_ext['clusters_3'] == 0].drop(['Id', 'Price', 'clusters_3'], axis = 1) , \n",
    "          train_ext.loc[train_ext['clusters_3'] == 0, 'Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr_1 = RandomForestRegressor(max_depth=18, max_features=5, n_estimators=320, random_state=29)\n",
    "rgr_1.fit(train_ext[train_ext['clusters_3'] == 1].drop(['Id', 'Price', 'clusters_3'], axis = 1) , \n",
    "          train_ext.loc[train_ext['clusters_3'] == 1, 'Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr_2 = RandomForestRegressor(max_depth=18, max_features=5, n_estimators=320, random_state=29)\n",
    "rgr_2.fit(train_ext[train_ext['clusters_3'] == 2].drop(['Id', 'Price', 'clusters_3'], axis = 1) , \n",
    "          train_ext.loc[train_ext['clusters_3'] == 2, 'Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog_predict_rfr_CV_kmeans_full_model_c0 = rgr_0.predict(test_ext[(test_ext['clusters_3'] == 0)].drop(['Id', 'clusters_3'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog_predict_rfr_CV_kmeans_full_model_c1 = rgr_1.predict(test_ext[(test_ext['clusters_3'] == 1)].drop(['Id', 'clusters_3'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog_predict_rfr_CV_kmeans_full_model_c2 = rgr_2.predict(test_ext[(test_ext['clusters_3'] == 2)].drop(['Id', 'clusters_3'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog_predict_rfr_CV_kmeans_cls_full_model = np.hstack([itog_predict_rfr_CV_kmeans_full_model_c0, \n",
    "                                                       itog_predict_rfr_CV_kmeans_full_model_c1, \n",
    "                                                       itog_predict_rfr_CV_kmeans_full_model_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ext['Price'] = itog_predict_rfr_CV_kmeans_cls_full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ext[['Id', 'Price']].to_csv('rfr_CV_kmeans_cls_full_model_rev07.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame()\n",
    "imp['name'] = test_ext.drop(['Id','Price', 'clusters_3'], axis = 1).columns\n",
    "imp['value'] = rgr_0.feature_importances_\n",
    "print(imp.sort_values('value' , ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame()\n",
    "imp['name'] = test_ext.drop(['Id','Price', 'clusters_3'], axis = 1).columns\n",
    "imp['value'] = rgr_1.feature_importances_\n",
    "print(imp.sort_values('value' , ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame()\n",
    "imp['name'] = test_ext.drop(['Id','Price', 'clusters_3'], axis = 1).columns\n",
    "imp['value'] = rgr_2.feature_importances_\n",
    "print(imp.sort_values('value' , ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaler**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(SCALER_FILE_PATH, 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(MODEL_FILE_PATH, 'wb') as file:\n",
    "    pickle.dump(final_model, file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(MODEL_FILE_PATH, 'rb') as file:\n",
    "    model_load=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
